{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a33da2-7f2c-452c-9732-f0741b35f2b3",
   "metadata": {},
   "source": [
    "# Beginner version - Using Web Scraping For Your Hobby Or:\n",
    "\n",
    "# One Of My Favourite Designers Anna Maria Horner\n",
    " \n",
    "\n",
    "In this notebook, I build a web scraper, which works with one search term - in my case \"Anna Maria Horner\". As the HTML structure differs from website to website, this scraper only works with the site which I scraped. But one can easily adapt it. \n",
    "\n",
    "The following example rather serves as an introduction: it contains some explanations and includes only one function in the end. Handle it as a proto type - the code is organized in an imperative style, in a series of statements.\n",
    "\n",
    "The other notebooks of this repo show a different approach, they work with functions which also include docstrings.\n",
    "\n",
    "\n",
    "The website - a patchwork shop in the UK - shows me the search result with 30 hits per page. The following scraper only gathers information from the first page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4739e34-49b7-4b9d-8d4e-ed1e692f7e9f",
   "metadata": {},
   "source": [
    "## First step: Respect Scraping Ethics!\n",
    "\n",
    "**The scraped content belongs to the website owner. Check robots.txt and the Terms of Use! If in doubt, scrape websites which have an API. Making requests to a website can cause a toll on a website's performance, please do not cause any disruption to the regular functioning of the website! (too many requests, huge requests...)**\n",
    "\n",
    "First step is always: Checking the onlineshop's robots.txt. It shows me that there are only a few things to note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb70bde-5f09-4035-9189-ef3380f6d285",
   "metadata": {},
   "source": [
    "User-agent: *\n",
    "\n",
    "Disallow: /wp-admin/\n",
    "\n",
    "Allow: /wp-admin/admin-ajax.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a00fa-d05e-40f8-aaea-f71dec47c27f",
   "metadata": {},
   "source": [
    "So, let's start and find some products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286bba9-d215-44b5-8cbc-6d9ca90d9686",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2702a7-c338-4ab3-92db-1ff402a818d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db764fa6-a053-411e-9ce0-81deeb8ead68",
   "metadata": {},
   "source": [
    "## Initiating request\n",
    "\n",
    "The requests library needs a functioning URL to the website which you would like to scrape. Mine includes the search term in the path/ get parameters. The easiest way to get the URL is to go to the website, find the page you want to scrape and copy the link. \n",
    "\n",
    "You don't have to set the timeout parameter, but be aware that the default ist very high. Which is why I often times find it set to 2 seconds. In my case he timeout had to be set to 5, after starting with 2, which threw a timeout error. \n",
    "\n",
    "The following code executes an HTTP request to the specified URL. It retrieves the HTML data that the server sends back and saves this data in a Python object: requests.Response().\n",
    "\n",
    "I am printing the status code, which prints out 200. Good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04a21a5-ed7c-4f1d-ab63-9ce9cc0ce731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "url = \"webpage you want to scrape\"\n",
    "\n",
    "page = requests.get(url, timeout=5)\n",
    "print(page.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64e9f61-a1b6-429d-a0d4-7d9c006d358f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783d242-c94e-4cce-ae62-524851dc0eef",
   "metadata": {},
   "source": [
    "The output is a requests.Response object. There is a number of key properties and methods available to a Response object. Above we saw .status_code which stores the integer code of the responded HTTP status, such as 200 or 404. \n",
    "\n",
    "In the following we are going to use .content - which contains the content of the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ba906-7a28-440d-80c5-1f78a1e8451b",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "What happened so far: I scraped HTML code from the website and it is stored in the requests.Response() object which I called page. The scraped code still contains a lot of HTML tags and attributes. That's why I would like to make the data more readable and pick out what interests me.\n",
    "\n",
    "With Beautiful Soup, another Python library, you can parse structured data (like HTML). The library provides a number of intuitive functions that you can use to analyze the HTML you receive. See the documentation here: https://beautiful-soup-4.readthedocs.io/en/latest/ . \n",
    "\n",
    "The following line of code creates a BeautifulSoup object that receives the HTML content as input. When the object is instantiated, you also instruct Beautiful Soup to use the corresponding parser. In the example below, this is the HTML parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25866a71-a9ef-4a6b-a604-a7a40bd4d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "annamaria = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d5cbe0-9e2f-48df-bffc-e33cf09491ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(annamaria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe46ba-d4e8-47b4-89a8-f1e97ea546f2",
   "metadata": {},
   "source": [
    "## Finding Each Product on the First Page\n",
    "\n",
    "How to look at the HTML code of a web page: Use the Inspect functionality (Firefox browser) on the repective page.\n",
    "\n",
    "How to look for certain parts of the web page in the HTML code: You can hover over the part that you are interested in. This will highlight the respective web page section. Use drop down in HTML inspect tool to isolate the part even more. You will observe HTML tags, classes and attributes, which can also be found in the BeautifulSoup object. With the library BeautifulSoup, one can interact with HTML in a similar way to how one can do using the inspect functionality.\n",
    "\n",
    "I found that each product can be detected by the tag div, filtered by class=\"product-element-bottom product-information\". Using find_all() to actually find all the individual products stored in the annamaria variable. \n",
    "\n",
    "Storing the result in a variable called products. There are maximal 30 products per page. We know that there are several result pages, so len(product) should give out 30. Which it does, yeah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df34b39-3bd9-415b-9f12-a1a418e732f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = annamaria.find_all(\"div\", {\"class\" : \"product-element-bottom product-information\"})\n",
    "len(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fcb4f54-7533-42dc-867d-3d53010cffe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5af37e-9dfe-4678-a415-6798eb62a435",
   "metadata": {},
   "source": [
    "## Checking out the first product\n",
    "\n",
    "As a starting point, I am interested in the product title, the price and the link to the specific product page. I can find all this information in my result, as the print out of the first product shows. \n",
    "\n",
    "Note: you can easily use slicing to access each of the 30 products stored in the variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585f0b8-ecc8-4d3b-932a-d51959e2fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "products[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b62ef-a99f-4524-8590-df40bc27410c",
   "metadata": {},
   "source": [
    "## Gathering product name and link to the specific product page\n",
    "\n",
    "To achieve that task, I just started with the first element in the products variable. I tested if my code obtains the right information by applying it on the second element in products as well. \n",
    "\n",
    "Product name and link can be extracted through the first anchor tag (\"a\"), the product name can be detected with the .get_text() function, the link to the product page is reached with .get(\"href\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6679e70-b93a-46b9-99b1-50a2a30836fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anna Maria’s Welcome Home Quilt Kit'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[0].find(\"a\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654d78fd-e3da-4cf0-a565-19588e698c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Made My Day Canna Toffee'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[1].find(\"a\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04e45c-b14d-496a-b9be-47d84dbe2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "products[0].find(\"a\").get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d48cddd-6429-45db-b0a3-5d0a9d052a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "products[1].find(\"a\").get(\"href\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea7567-9b32-4276-b6dc-193ac2321215",
   "metadata": {},
   "source": [
    "## Searching for the price\n",
    "\n",
    "To be able to extract the price, I had to try a few things out. First, I used the \"span\" tag with class=price. One can see that the price and currency is in there. With indexing and the .get_text() function, I could fetch currency and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab00bbb5-845d-4cc1-9278-86185210109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"price\"><span class=\"woocommerce-Price-amount amount\"><bdi><span class=\"woocommerce-Price-currencySymbol\">£</span>285.00</bdi></span></span>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[0].find_all(\"span\", {\"class\" : \"price\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b591f72a-08c6-465a-84e6-d357cd81c181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'£285.00'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[0].find_all(\"span\", {\"class\" : \"price\"})[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab8c64f-5724-46ef-97ed-2881fcec5df6",
   "metadata": {},
   "source": [
    "Another possibility is to search for the \"bdi\" tag. The result is a bit narrower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e9238f1-6580-463b-8c58-c028a4df8682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bdi><span class=\"woocommerce-Price-currencySymbol\">£</span>285.00</bdi>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[0].find(\"bdi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f3eb64-12c3-41ba-9805-0e079dd1290b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'£285.00'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[0].find(\"bdi\").get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee4442-a004-4549-ac68-04e856738305",
   "metadata": {},
   "source": [
    "One might want to separate currency symbol from number. Of course, everything is in pounds on this page. This can later be accounted for in the column name, if it is of interest. Also, one can transform the price from string type to float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a04b7d-a7c4-45be-a623-7ef15a61ab12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = products[0].find_all(\"bdi\")[0].get_text()\n",
    "price = float(price.split(\"£\")[1])\n",
    "price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d912d-e0ba-4529-b617-918686ece7a0",
   "metadata": {},
   "source": [
    "## Creating a DataFrame with all the products\n",
    "\n",
    "Beside product name, product page and prize, I wanted to add another variable: product category. The information can be found in the product name, where I want to start. An alternative would be of course to check the product page for any information on that. But let's start with the result page entries and the information we can get there. \n",
    "\n",
    "I created a function find_product_category() where I assigned each product to a product category, depending on wether the respective keyword pointing to a certain category was included in the product name or not. I purposely did not split the product name afterwards, and left it as is. Just wanted to create a column which I can easily browse for the different product categories. I included my function into my loop, which will step by step create my DataFrame (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9b2af28-d121-4a9d-aa72-c66585bd83dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_product_category(product_name):\n",
    "    if \"Fat Quarter Bundle\" in product_name:\n",
    "        product_category = \"Fat Quarter Bundle\"\n",
    "    elif \"Kit\" in product_name:\n",
    "        product_category = \"Quilt Kit\"\n",
    "    elif \"Pattern\" in product_name:\n",
    "        product_category = \"Pattern\"\n",
    "    elif \"Thread\" in product_name:\n",
    "        product_category = \"Thread\"\n",
    "    elif \"Template\" in product_name:\n",
    "        product_category = \"Templates\"\n",
    "    else:\n",
    "        product_category = \"Bulk Goods\"\n",
    "\n",
    "    return product_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58883950-473b-4e06-aff5-1c633d4d9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = []\n",
    "\n",
    "for product in products:\n",
    "\n",
    "    try:\n",
    "        product_name = product.find(\"a\").get_text()\n",
    "    except AttributeError:\n",
    "        product_name = None\n",
    "\n",
    "    product_category = find_product_category(product_name)\n",
    "    \n",
    "    try:\n",
    "        product_page = product.find(\"a\").get(\"href\")\n",
    "    except AttributeError:\n",
    "        product_page = None\n",
    "\n",
    "    try:\n",
    "        price_in_gbp = product.find(\"bdi\").get_text()\n",
    "        price_in_gbp = float(price_in_gbp.split(\"£\")[1])\n",
    "    except AttributeError:\n",
    "        price_in_gbp = None\n",
    "\n",
    "\n",
    "    product_list.append({\n",
    "                \"Product Name\": product_name,\n",
    "                \"Product Category\" : product_category,\n",
    "                \"Product Page\": product_page,\n",
    "                \"Price in GBP\": price_in_gbp\n",
    "            })\n",
    "    \n",
    "df = pd.DataFrame(product_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd68908-8615-4caf-b3c9-ca78e222ba47",
   "metadata": {},
   "source": [
    "## Write DataFrame to csv file\n",
    "\n",
    "I am storing the DataFrame as a csv file, as I would like to use it later on in an other project. You can store the csv file right within the same folder by just using the df.to_csv() functionality. Revert to Pandas documentation for parameters. I used another approach.\n",
    "\n",
    "I want to add the csv file to a seperate folder called data. The path to my folder is stored in a text file - path_to_data.txt. You obviously don't have to do that. I extract the path, add it to the path parameter of to_csv() using os.path.join().\n",
    "\n",
    "\n",
    "I added the path_to_date.txt file to my gitignore. If needed, one can add the data folder to .gitignore file as well to avoid pushing it to GitHub.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "319e7ad2-e018-4fb3-b16b-141082478ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d000e35-9653-4116-983e-d2735950db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('path_to_data.txt') as file:\n",
    "    path=file.readlines()\n",
    "path = path[0].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce9e88c3-6ce4-47e7-89ee-a3e8f4a68715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(path,r'amh_first_result_page.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ba600-62c3-42fe-91da-98d7c3c3aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if one does not want to follow above steps to store DataFrame in extra folder\n",
    "# uncomment the collowing to store csv in same working folder\n",
    "\n",
    "#df.to_csv('amh_first_result_page.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f6b26-5f50-4e9c-9209-389e404b4737",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "\n",
    "Two things can be done on top. One is to iterate through each single product page of my search and thereby storing all the available products plus their info. The other is, that one can access the actual product page and extract even more details/ info on each product."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
